{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYOQOLPucdpY19uiBCjtTj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marjan-Salari/Detection-social-anxiety-disorder-in-X/blob/main/Untitled147.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vry_6U0VuJmI"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# نصب کتابخانه‌های لازم\n",
        "# ===============================\n",
        "!pip install xgboost scikit-learn matplotlib seaborn gensim openpyxl --quiet\n",
        "\n",
        "# ===============================\n",
        "# ایمپورت کتابخانه‌ها\n",
        "# ===============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.utils import resample\n",
        "from gensim.models import Word2Vec\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# ===============================\n",
        "# مرحله 1: بارگذاری فایل‌ها\n",
        "# ===============================\n",
        "print(\"لطفا فایل آموزش (train) را انتخاب کنید:\")\n",
        "train_file = files.upload()\n",
        "train_filename = list(train_file.keys())[0]\n",
        "\n",
        "print(\"لطفا فایل اعتبارسنجی (validation) را انتخاب کنید:\")\n",
        "val_file = files.upload()\n",
        "val_filename = list(val_file.keys())[0]\n",
        "\n",
        "print(\"لطفا فایل تست (test) را انتخاب کنید:\")\n",
        "test_file = files.upload()\n",
        "test_filename = list(test_file.keys())[0]\n",
        "\n",
        "def load_file(filename):\n",
        "    if filename.endswith('.csv'):\n",
        "        return pd.read_csv(filename)\n",
        "    elif filename.endswith(('.xlsx', '.xls')):\n",
        "        return pd.read_excel(filename)\n",
        "    else:\n",
        "        raise ValueError(\"File must be CSV or Excel\")\n",
        "\n",
        "train_df = load_file(train_filename)\n",
        "val_df = load_file(val_filename)\n",
        "test_df = load_file(test_filename)\n",
        "\n",
        "# ===============================\n",
        "# مرحله 2: توازن کلاس‌ها در دیتای آموزش\n",
        "# ===============================\n",
        "def balance_data(df, target='label'):\n",
        "    class_counts = df[target].value_counts()\n",
        "    if abs(class_counts[0] - class_counts[1]) / max(class_counts) > 0.1:\n",
        "        df_majority = df[df[target]==0]\n",
        "        df_minority = df[df[target]==1]\n",
        "        df_majority_downsampled = resample(df_majority,\n",
        "                                           replace=False,\n",
        "                                           n_samples=len(df_minority),\n",
        "                                           random_state=42)\n",
        "        df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
        "        return df_balanced.sample(frac=1, random_state=42)\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "train_df = balance_data(train_df)\n",
        "\n",
        "# ===============================\n",
        "# مرحله 3: آماده‌سازی داده‌ها با حفظ توکن‌ها\n",
        "# ===============================\n",
        "X_train = train_df['text']\n",
        "y_train = train_df['label']\n",
        "\n",
        "X_val = val_df['text']\n",
        "y_val = val_df['label']\n",
        "\n",
        "X_test = test_df['text']\n",
        "y_test = test_df['label'] if 'label' in test_df.columns else None\n",
        "\n",
        "# برای Vectorizer موقتا لیست توکن را به رشته تبدیل می‌کنیم\n",
        "def tokens_to_string(tokens):\n",
        "    if isinstance(tokens, list):\n",
        "        return \" \".join(tokens)\n",
        "    else:\n",
        "        return str(tokens)\n",
        "\n",
        "X_train_temp = X_train.apply(tokens_to_string)\n",
        "X_val_temp = X_val.apply(tokens_to_string)\n",
        "X_test_temp = X_test.apply(tokens_to_string)\n",
        "\n",
        "# ===============================\n",
        "# مرحله 4: تعریف ویژگی‌ها\n",
        "# ===============================\n",
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_temp)\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val_temp)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_temp)\n",
        "\n",
        "# N-gram (CountVectorizer)\n",
        "ngram_vectorizer = CountVectorizer(max_features=5000, ngram_range=(1,2))\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train_temp)\n",
        "X_val_ngram = ngram_vectorizer.transform(X_val_temp)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test_temp)\n",
        "\n",
        "# Skip-gram (Word2Vec متوسط بردار کلمات)\n",
        "def get_w2v_embeddings(texts, size=100, window=5, min_count=1):\n",
        "    sentences = [t if isinstance(t, list) else str(t).split() for t in texts]\n",
        "    w2v_model = Word2Vec(sentences, vector_size=size, window=window, min_count=min_count, workers=4)\n",
        "    embeddings = []\n",
        "    for sent in sentences:\n",
        "        vecs = [w2v_model.wv[word] for word in sent if word in w2v_model.wv]\n",
        "        if len(vecs) > 0:\n",
        "            embeddings.append(np.mean(vecs, axis=0))\n",
        "        else:\n",
        "            embeddings.append(np.zeros(size))\n",
        "    return np.array(embeddings)\n",
        "\n",
        "X_train_w2v = get_w2v_embeddings(X_train)\n",
        "X_val_w2v = get_w2v_embeddings(X_val)\n",
        "X_test_w2v = get_w2v_embeddings(X_test)\n",
        "\n",
        "# ===============================\n",
        "# مرحله 5: تعریف مدل‌ها\n",
        "# ===============================\n",
        "models = {\n",
        "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
        "    'SVM': SVC(probability=True),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "features = {\n",
        "    'TF-IDF': (X_train_tfidf, X_val_tfidf, X_test_tfidf),\n",
        "    'N-gram': (X_train_ngram, X_val_ngram, X_test_ngram),\n",
        "    'Skip-gram': (X_train_w2v, X_val_w2v, X_test_w2v)\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "# پوشه برای ذخیره تصاویر\n",
        "os.makedirs(\"plots\", exist_ok=True)\n",
        "\n",
        "# ===============================\n",
        "# مرحله 6: آموزش و ارزیابی\n",
        "# ===============================\n",
        "for feat_name, (X_tr, X_vl, X_te) in features.items():\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\n=== Model: {model_name} | Feature: {feat_name} ===\")\n",
        "        model.fit(X_tr, y_train)\n",
        "        y_pred_val = model.predict(X_vl)\n",
        "\n",
        "        acc = accuracy_score(y_val, y_pred_val)\n",
        "        prec = precision_score(y_val, y_pred_val)\n",
        "        rec = recall_score(y_val, y_pred_val)\n",
        "        f1 = f1_score(y_val, y_pred_val)\n",
        "        cm = confusion_matrix(y_val, y_pred_val)\n",
        "\n",
        "        print(f\"Accuracy: {acc:.4f}\")\n",
        "        print(f\"Precision: {prec:.4f}\")\n",
        "        print(f\"Recall: {rec:.4f}\")\n",
        "        print(f\"F1-score: {f1:.4f}\")\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(cm)\n",
        "\n",
        "        # ذخیره نتایج\n",
        "        results.append({\n",
        "            'Model': model_name,\n",
        "            'Feature': feat_name,\n",
        "            'Accuracy': acc,\n",
        "            'Precision': prec,\n",
        "            'Recall': rec,\n",
        "            'F1-score': f1\n",
        "        })\n",
        "\n",
        "        # ذخیره نمودار ماتریس\n",
        "        plt.figure(figsize=(5,4))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "        plt.title(f'Confusion Matrix: {model_name} | {feat_name}')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.savefig(f'plots/ConfusionMatrix_{model_name}_{feat_name}.png')\n",
        "        plt.close()\n",
        "\n",
        "# ===============================\n",
        "# مرحله 7: ذخیره نتایج و نمودارها\n",
        "# ===============================\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"model_results.csv\", index=False)\n",
        "results_df.to_excel(\"model_results.xlsx\", index=False)\n",
        "\n",
        "# ذخیره نتایج در فایل TXT\n",
        "with open(\"model_results.txt\", \"w\") as f:\n",
        "    for r in results:\n",
        "        f.write(f\"Model: {r['Model']} | Feature: {r['Feature']} | Accuracy: {r['Accuracy']:.4f} | Precision: {r['Precision']:.4f} | Recall: {r['Recall']:.4f} | F1-score: {r['F1-score']:.4f}\\n\")\n",
        "\n",
        "# نمودار مقایسه‌ای خطی و میله‌ای\n",
        "plt.figure(figsize=(12,6))\n",
        "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-score']:\n",
        "    for feat_name in features.keys():\n",
        "        subset = results_df[results_df['Feature']==feat_name]\n",
        "        plt.plot(subset['Model'], subset[metric], marker='o', label=f\"{metric}-{feat_name}\")\n",
        "plt.title(\"Model Comparison Line Chart\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"plots/Model_Comparison_Line.png\")\n",
        "plt.close()\n",
        "\n",
        "# نمودار میله‌ای\n",
        "plt.figure(figsize=(12,6))\n",
        "metrics_plot = results_df.melt(id_vars=['Model','Feature'], value_vars=['Accuracy','Precision','Recall','F1-score'], var_name='Metric', value_name='Score')\n",
        "sns.barplot(x='Model', y='Score', hue='Feature', data=metrics_plot)\n",
        "plt.title(\"Model Comparison Bar Chart\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.savefig(\"plots/Model_Comparison_Bar.png\")\n",
        "plt.close()\n",
        "\n",
        "# ===============================\n",
        "# مرحله 8: دانلود همه فایل‌ها\n",
        "# ===============================\n",
        "files.download(\"model_results.csv\")\n",
        "files.download(\"model_results.xlsx\")\n",
        "files.download(\"model_results.txt\")\n",
        "files.download(\"plots/Model_Comparison_Line.png\")\n",
        "files.download(\"plots/Model_Comparison_Bar.png\")\n",
        "\n",
        "# دانلود Confusion Matrix ها\n",
        "for file in os.listdir(\"plots\"):\n",
        "    if \"ConfusionMatrix\" in file:\n",
        "        files.download(os.path.join(\"plots\", file))\n",
        "\n",
        "print(\"All results and plots are saved and ready for download.\")\n"
      ]
    }
  ]
}